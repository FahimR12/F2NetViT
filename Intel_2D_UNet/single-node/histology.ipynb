{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Histology Demo\n",
    "\n",
    "This demo uses the [colorectal histology images dataset](https://www.tensorflow.org/datasets/catalog/colorectal_histology) to train a simple convolutional neural network in TensorFlow. \n",
    "\n",
    "All images are RGB, 0.495 µm per pixel, digitized with an Aperio ScanScope (Aperio/Leica biosystems), magnification 20x. Histological samples are fully anonymized images of formalin-fixed paraffin-embedded human colorectal adenocarcinomas (primary tumors) from our pathology archive (Institute of Pathology, University Medical Center Mannheim, Heidelberg University, Mannheim, Germany)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zenodo.org/record/53169#.X1bMe3lKguX\n",
    "Kather, J. N., Zöllner, F. G., Bianconi, F., Melchers, S. M., Schad, L. R., Gaiser, T., … Weis, C.-A. (2016). Collection of textures in colorectal cancer histology [Data set]. Zenodo. http://doi.org/10.5281/zenodo.53169\n",
    "\n",
    "Kather JN, Weis CA, Bianconi F, Melchers SM, Schad LR, Gaiser T, Marx A, Zollner F: Multi-class texture analysis in colorectal cancer histology (2016), Scientific Reports (in press)\n",
    "\n",
    "@article{kather2016multi,\n",
    "  title={Multi-class texture analysis in colorectal cancer histology},\n",
    "  author={Kather, Jakob Nikolas and Weis, Cleo-Aron and Bianconi, Francesco and Melchers, Susanne M and Schad, Lothar R and Gaiser, Timo and Marx, Alexander and Z{\"o}llner, Frank Gerrit},\n",
    "  journal={Scientific reports},\n",
    "  volume={6},\n",
    "  pages={27988},\n",
    "  year={2016},\n",
    "  publisher={Nature Publishing Group}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow_datasets in ./.local/lib/python3.7/site-packages (3.2.1)\n",
      "Requirement already satisfied: termcolor in ./.local/lib/python3.7/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /glob/development-tools/versions/oneapi/beta09/inteloneapi/intelpython/python3.7/lib/python3.7/site-packages (from tensorflow_datasets) (20.2.0)\n",
      "Requirement already satisfied: tensorflow-metadata in ./.local/lib/python3.7/site-packages (from tensorflow_datasets) (0.24.0)\n",
      "Requirement already satisfied: future in ./.local/lib/python3.7/site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: tqdm in /glob/development-tools/versions/oneapi/beta09/inteloneapi/intelpython/python3.7/lib/python3.7/site-packages (from tensorflow_datasets) (4.39.0)\n",
      "Requirement already satisfied: wrapt in ./.local/lib/python3.7/site-packages (from tensorflow_datasets) (1.12.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /glob/development-tools/versions/oneapi/beta09/inteloneapi/intelpython/python3.7/lib/python3.7/site-packages (from tensorflow_datasets) (2.23.0)\n",
      "Requirement already satisfied: promise in ./.local/lib/python3.7/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: absl-py in ./.local/lib/python3.7/site-packages (from tensorflow_datasets) (0.10.0)\n",
      "Requirement already satisfied: six in /glob/development-tools/versions/oneapi/beta09/inteloneapi/intelpython/python3.7/lib/python3.7/site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: dill in ./.local/lib/python3.7/site-packages (from tensorflow_datasets) (0.3.2)\n",
      "Requirement already satisfied: numpy in /glob/development-tools/versions/oneapi/beta09/inteloneapi/intelpython/python3.7/lib/python3.7/site-packages (from tensorflow_datasets) (1.18.5)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in ./.local/lib/python3.7/site-packages (from tensorflow_datasets) (3.13.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in ./.local/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.52.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /glob/development-tools/versions/oneapi/beta09/inteloneapi/intelpython/python3.7/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /glob/development-tools/versions/oneapi/beta09/inteloneapi/intelpython/python3.7/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /glob/development-tools/versions/oneapi/beta09/inteloneapi/intelpython/python3.7/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /glob/development-tools/versions/oneapi/beta09/inteloneapi/intelpython/python3.7/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.6.20)\n",
      "Requirement already satisfied: setuptools in /glob/development-tools/versions/oneapi/beta09/inteloneapi/intelpython/python3.7/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow_datasets) (49.6.0.post20200826)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets   # Install TensorFlow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:        x86_64\n",
      "CPU op-mode(s):      32-bit, 64-bit\n",
      "Byte Order:          Little Endian\n",
      "CPU(s):              24\n",
      "On-line CPU(s) list: 0-23\n",
      "Thread(s) per core:  2\n",
      "Core(s) per socket:  6\n",
      "Socket(s):           2\n",
      "NUMA node(s):        2\n",
      "Vendor ID:           GenuineIntel\n",
      "CPU family:          6\n",
      "Model:               85\n",
      "Model name:          Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz\n",
      "Stepping:            4\n",
      "CPU MHz:             1200.417\n",
      "CPU max MHz:         3700.0000\n",
      "CPU min MHz:         1200.0000\n",
      "BogoMIPS:            6800.00\n",
      "Virtualization:      VT-x\n",
      "L1d cache:           32K\n",
      "L1i cache:           32K\n",
      "L2 cache:            1024K\n",
      "L3 cache:            19712K\n",
      "NUMA node0 CPU(s):   0-5,12-17\n",
      "NUMA node1 CPU(s):   6-11,18-23\n",
      "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke md_clear flush_l1d\n"
     ]
    }
   ],
   "source": [
    "# Determine what type of CPU we are using\n",
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7776db955d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# export TF_DISABLE_MKL=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TF_DISABLE_MKL\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m\"0\"\u001b[0m  \u001b[0;31m# Disable Intel optimizations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# export MKLDNN_VERBOSE=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#os.environ[\"MKLDNN_VERBOSE\"]  = \"1\"     # 1 = Print log statements; 0 = silent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# export TF_DISABLE_MKL=1\n",
    "os.environ[\"TF_DISABLE_MKL\"]  = \"0\"  # Disable Intel optimizations\n",
    "\n",
    "# export MKLDNN_VERBOSE=1\n",
    "#os.environ[\"MKLDNN_VERBOSE\"]  = \"1\"     # 1 = Print log statements; 0 = silent\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"12\"   # Number of physical cores\n",
    "os.environ[\"KMP_BLOCKTIME\"]   = \"1\"    \n",
    "\n",
    "# If hyperthreading is enabled, then use\n",
    "os.environ[\"KMP_AFFINITY\"]    = \"granularity=thread,compact,1,0\"\n",
    "\n",
    "# If hyperthreading is NOT enabled, then use\n",
    "#os.environ[\"KMP_AFFINITY\"]   = \"granularity=thread,compact\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version = {}\".format(tf.__version__))\n",
    "print(\"Does TensorFlow have the Intel optimizations: {}\".format(tf.python._pywrap_util_port.IsMklEnabled()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: The download is broken on Windows due to the long filenames in the download. You'll have to manually download and extract on a Windows machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds), ds_info =  tfds.load('colorectal_histology', data_dir=\".\", \n",
    "                                          shuffle_files=True, split='train', \n",
    "                                          with_info=True, as_supervised=True)\n",
    "\n",
    "assert isinstance(ds, tf.data.Dataset)\n",
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a few examples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_key, y_key = ds_info.supervised_keys\n",
    "ds_temp = ds.map(lambda x, y: {x_key: x, y_key: y})\n",
    "tfds.show_examples(ds_temp, ds_info, plot_scale=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are the 8 labels for the histology classification subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info.features['label'].names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the data loaders\n",
    "\n",
    "This will also do online data augmentation by randomly flipping the images (up/down, left/right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = ds_info.splits['train'].num_examples\n",
    "train_split_percentage = 0.80\n",
    "train_batch_size = 128\n",
    "test_batch_size = 16\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "def augment_img(image, label):\n",
    "    \"\"\"Augment images: `uint8` -> `float32`.\"\"\"\n",
    "    \n",
    "    image = tf.image.random_flip_left_right(image) # Random flip Left/Right\n",
    "    image = tf.image.random_flip_up_down(image)    # Random flip Up/Down\n",
    "    \n",
    "    return tf.cast(image, tf.float32) / 255., label # Normalize 0 to 1 for pixel values\n",
    "\n",
    "# Get train dataset\n",
    "ds_train = ds.take(int(n * train_split_percentage))\n",
    "ds_train = ds_train.map(\n",
    "    augment_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(int(n * train_split_percentage))\n",
    "ds_train = ds_train.batch(train_batch_size)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Get test dataset\n",
    "ds_test = ds.skip(int(n * train_split_percentage)).map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.batch(test_batch_size)\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "Here's a Convolutional neural network model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=ds_info.features['image'].shape)\n",
    "conv = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "conv = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv)\n",
    "maxpool = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(conv)\n",
    "\n",
    "conv = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(maxpool)\n",
    "conv = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv)\n",
    "concat = tf.keras.layers.concatenate([maxpool, conv])\n",
    "maxpool = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(concat)\n",
    "\n",
    "conv = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(maxpool)\n",
    "conv = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv)\n",
    "concat = tf.keras.layers.concatenate([maxpool, conv])\n",
    "maxpool = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(concat)\n",
    "\n",
    "conv = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(maxpool)\n",
    "conv = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv)\n",
    "concat = tf.keras.layers.concatenate([maxpool, conv])\n",
    "maxpool = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(concat)\n",
    "\n",
    "flat = tf.keras.layers.Flatten()(maxpool)\n",
    "dense = tf.keras.layers.Dense(128)(flat)\n",
    "drop = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\n",
    "predict = tf.keras.layers.Dense(ds_info.features['label'].num_classes)(drop)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[inputs], outputs=[predict])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=[tf.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Checkpoints\n",
    "\n",
    "Save the model that performs best on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model\n",
    "model_dir = \"checkpoints\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=model_dir, \n",
    "                                                         save_best_only=True,\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "Stop training if the validation doesn't improve after a certain number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callback for Early Stopping of training\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=8) # Stop once validation loss plateaus for patience epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding TensorBoard\n",
    "\n",
    "Adding TensorBoard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard logs\n",
    "import datetime\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call *fit* to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs):\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        epochs=epochs,     \n",
    "        validation_data=ds_test,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback, tb_callback]\n",
    "    )\n",
    "    return history\n",
    "    \n",
    "epochs = 5   # Run for this many epochs - Increase if you have some time\n",
    "history = train_model(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8));\n",
    "plt.plot(range(1,epochs+1), history.history['sparse_categorical_accuracy'], '.-');\n",
    "plt.plot(range(1,epochs+1), history.history['val_sparse_categorical_accuracy'], '.-');\n",
    "plt.legend(['training', 'validation'], fontsize=20);\n",
    "plt.xticks(size=20);\n",
    "plt.xlabel('Epochs', size=20);\n",
    "plt.yticks(size=20);\n",
    "plt.ylabel('Accuracy', size=20);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model that had the highest score on the validation dataset\")\n",
    "model = tf.keras.models.load_model(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the best model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating the best model on the test dataset\")\n",
    "_, accuracy = model.evaluate(ds_test)\n",
    "print(\"\\nModel accuracy on test dataset = {:.1f}%\".format(100.0*accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display some predictions on the test data\n",
    "\n",
    "We grab a random subset of the test dataset and plot the image along with the ground truth label, the TensorFlow model prediction, and the OpenVINO model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tfds.as_numpy(ds_test.shuffle(100).take(1)) # Take 1 random batch\n",
    "\n",
    "for image, label in test_data:\n",
    "    num = 8 # len(label)\n",
    "    cols = 2\n",
    "    plt.figure(figsize=(25,25))\n",
    "    \n",
    "    for idx in range(num):\n",
    "        \n",
    "        plt.subplot(int(np.ceil(num/cols)), cols, idx+1)\n",
    "        plt.imshow(image[idx])\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # TensorFlow model prediction\n",
    "        tf_predict = ds_info.features['label'].names[model.predict(image[[idx]]).argmax()]\n",
    "        \n",
    "        plt.title(\"Truth = {}\\nTensorFlow Predict = {}\".format(ds_info.features['label'].names[label[idx]], tf_predict))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow (AI kit)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

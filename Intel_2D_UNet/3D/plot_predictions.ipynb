{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing OpenVINO&trade; to TensorFlow Predictions\n",
    "\n",
    "In this notebook, we'll go through how to perform predictions on our 3D U-Net using both TensorFlow and OpenVINO&trade;.  You should be able to see that OpenVINO&trade; inference gives a significant speedup to these predictions.\n",
    "\n",
    "We'll assume that you already ran `train.py` and have trained a TensorFlow 3D U-Net model on the BraTS Medical Decathlon dataset.  We'll further assume that you have converted the final TensorFlow 3D U-Net model to OpenVINO&trade; by running something like:\n",
    "\n",
    "```\n",
    "source /opt/intel/openvino/bin/setupvars.sh\n",
    "python $INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo_tf.py \\\n",
    "       --saved_model_dir 3d_unet_decathlon_final \\\n",
    "       --model_name 3d_unet_decathlon \\\n",
    "       --batch 1  \\\n",
    "       --output_dir openvino_models/FP32 \\\n",
    "       --data_type FP32\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the OpenVINO&trade; Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some other Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "Note: We'll reuse the same data loader we used in training. Nevertheless, all we need to do is to provide the 3D MRI scan with the same preprocessing (normalization, cropping, etc) as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import DatasetGenerator\n",
    "import settings\n",
    "\n",
    "crop_dim = (settings.TILE_HEIGHT, settings.TILE_WIDTH,\n",
    "            settings.TILE_DEPTH, settings.NUMBER_INPUT_CHANNELS)\n",
    "\n",
    "settings.BATCH_SIZE = 1 \n",
    "\n",
    "brats_data = DatasetGenerator(crop_dim=crop_dim,\n",
    "                              data_path=settings.DATA_PATH,\n",
    "                              batch_size=settings.BATCH_SIZE,\n",
    "                              train_test_split=settings.TRAIN_TEST_SPLIT,\n",
    "                              validate_test_split=settings.VALIDATE_TEST_SPLIT,\n",
    "                              number_output_classes=settings.NUMBER_OUTPUT_CLASSES,\n",
    "                              random_seed=settings.RANDOM_SEED)\n",
    "\n",
    "brats_data.print_info()  # Print dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the OpenVINO&trade; model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openvino_filename = \"openvino_models/FP32/3d_unet_decathlon\"\n",
    "path_to_xml_file = \"{}.xml\".format(openvino_filename)\n",
    "path_to_bin_file = \"{}.bin\".format(openvino_filename)\n",
    "\n",
    "ie = IECore()\n",
    "net = ie.read_network(model=path_to_xml_file, weights=path_to_bin_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the OpenVINO&trade; model to the hardware device\n",
    "\n",
    "In this case our device is `CPU`. We could also use `MYRIAD` for the Intel&reg; NCS2&trade; VPU or `GPU` for the Intel&reg; GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_name = next(iter(net.input_info))\n",
    "output_layer_name = next(iter(net.outputs))\n",
    "print(\"Input layer name = {}\\nOutput layer name = {}\".format(input_layer_name, output_layer_name))\n",
    "\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\", num_requests=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the final TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf_model = tf.keras.models.load_model(\"3d_unet_decathlon_final\", compile=False)\n",
    "tf_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_intel_tensorflow():\n",
    "    \"\"\"\n",
    "    Check if Intel version of TensorFlow is installed\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "\n",
    "    print(\"We are using Tensorflow version {}\".format(tf.__version__))\n",
    "\n",
    "    major_version = int(tf.__version__.split(\".\")[0])\n",
    "    if major_version >= 2:\n",
    "        from tensorflow.python import _pywrap_util_port\n",
    "        print(\"Intel-optimizations (DNNL) enabled:\",\n",
    "              _pywrap_util_port.IsMklEnabled())\n",
    "    else:\n",
    "        print(\"Intel-optimizations (DNNL) enabled:\",\n",
    "              tf.pywrap_tensorflow.IsMklEnabled())\n",
    "\n",
    "\n",
    "test_intel_tensorflow()  # Prints if Intel-optimized TensorFlow is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Dice coefficient\n",
    "\n",
    "This measures the performance of the model from 0 to 1 where 1 means the model gives a perfect prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dice(target, prediction, smooth=0.0001):\n",
    "    \"\"\"\n",
    "    Sorenson Dice\n",
    "    \"\"\"\n",
    "    prediction = np.round(prediction)\n",
    "\n",
    "    numerator = 2.0 * np.sum(target * prediction) + smooth\n",
    "    denominator = np.sum(target) + np.sum(prediction) + smooth\n",
    "    coef = numerator / denominator\n",
    "\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the predictions for both OpenVINO&trade; and TensorFlow\n",
    "\n",
    "We'll also time the inference to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(img, msk):\n",
    "    \n",
    "    slicenum=np.argmax(np.sum(msk, axis=(1,2)))  # Find the slice with the largest tumor section\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.title(\"MRI\", fontsize=20)\n",
    "    plt.imshow(img[0,:,:,slicenum,0], cmap=\"bone\")\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(msk[0,:,:,slicenum,0], cmap=\"bone\")\n",
    "    plt.title(\"Ground truth\", fontsize=20)\n",
    "\n",
    "    \"\"\"\n",
    "    OpenVINO Model Prediction\n",
    "    Note: OpenVINO assumes the input (and output) are organized as channels first (NCHWD)\n",
    "    whereas TensorFlow assumes channels last (NHWDC). We'll use the NumPy transpose\n",
    "    to change the order.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    res = exec_net.infer({input_layer_name: np.transpose(img, [0,4,1,2,3])})\n",
    "    prediction_ov = np.transpose(res[output_layer_name], [0,2,3,4,1])    \n",
    "    print(\"OpenVINO inference time = {:.4f} msecs\".format(1000.0*(time.time()-start_time)))\n",
    "\n",
    "    plt.subplot(1,4,3)\n",
    "    dice_coef_ov = calc_dice(msk,prediction_ov)\n",
    "    plt.imshow(prediction_ov[0,:,:,slicenum,0], cmap=\"bone\")\n",
    "    plt.title(\"OpenVINO Prediction\\nDice = {:.4f}\".format(dice_coef_ov), fontsize=20)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    TensorFlow Model Prediction\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    prediction_tf = tf_model.predict(img)\n",
    "    print(\"TensorFlow inference time = {:.4f} msecs\".format(1000.0*(time.time()-start_time)))\n",
    "    \n",
    "    plt.subplot(1,4,4)\n",
    "    dice_coef_tf = calc_dice(msk,prediction_tf)\n",
    "    plt.imshow(prediction_tf[0,:,:,slicenum,0], cmap=\"bone\")\n",
    "    plt.title(\"TensorFlow Prediction\\nDice = {:.4f}\".format(dice_coef_tf), fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time\n",
    "\n",
    "Let's grab some data, perform inference, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = brats_data.get_test().take(1).as_numpy_iterator()\n",
    "for img, msk in ds:\n",
    "    plot_predictions(img,msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = brats_data.get_test().take(1).as_numpy_iterator()\n",
    "for img, msk in ds:\n",
    "    plot_predictions(img,msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = brats_data.get_test().take(1).as_numpy_iterator()\n",
    "for img, msk in ds:\n",
    "    plot_predictions(img,msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = brats_data.get_test().take(1).as_numpy_iterator()\n",
    "for img, msk in ds:\n",
    "    plot_predictions(img,msk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. SPDX-License-Identifier: EPL-2.0*\n",
    "\n",
    "*Copyright (c) 2019-2020 Intel Corporation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dataset if it has all the files for validation purposes\n",
    "if it miss a modality or any label then it will create problem during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1350 cases\n",
      "INFO: No issue found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse \n",
    "from utils.check_dataset import check\n",
    "!python utils/check_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load these YAML files in your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Config: {'roi': 128, 'infer_overlap': 0.5}\n",
      "Dataset Config: {'a_test_patient': '/BraTS-GLI-03064-100', 'dataset_xlsx': '/Dataset/BraTS2023_2017_GLI_Mapping.xlsx', 'json_file': '/Dataset/dataset.json', 'root_dir': '/home/fahim/F2NetViT', 'train_sub_dir': '/Dataset/BraTS2024-BraTS-GLI-TrainingData/training_data1_v2', 'validation_sub_dir': '/Dataset/BraTS2024-BraTS-GLI-ValidationData'}\n",
      "Training Config: {'batch_size': 1, 'num_workers': 8, 'fold': 0, 'infer_overlap': 0.6, 'learning_rate': 0.0001, 'max_epochs': 10, 'pretrained_model': '', 'seed': 50, 'sw_batch_size': 1, 'val_every': 2, 'weight_decay': 1e-05}\n",
      "Paths Config: {'dataset_file': '/home/fahim/F2NetViT/Dataset/BraTS2023_2017_GLI_Mapping.xlsx', 'json_file': '/home/fahim/F2NetViT/Dataset/dataset.json', 'test_patient': '/home/fahim/F2NetViT/Dataset/BraTS2024-BraTS-GLI-TrainingData/training_data1_v2/BraTS-GLI-03064-100/', 'train_path': '/home/fahim/F2NetViT/Dataset/BraTS2024-BraTS-GLI-TrainingData/training_data1_v2/', 'validation_path': '/home/fahim/F2NetViT/Dataset/BraTS2024-BraTS-GLI-ValidationData/'}\n",
      "Model Config: {'roi': 128, 'infer_overlap': 0.5}\n",
      "Dataset Config: {'a_test_patient': '/BraTS-GLI-03064-100', 'dataset_xlsx': '/Dataset/BraTS2023_2017_GLI_Mapping.xlsx', 'json_file': '/Dataset/dataset.json', 'root_dir': '/home/fahim/F2NetViT', 'train_sub_dir': '/Dataset/BraTS2024-BraTS-GLI-TrainingData/training_data1_v2', 'validation_sub_dir': '/Dataset/BraTS2024-BraTS-GLI-ValidationData'}\n",
      "Training Config: {'batch_size': 1, 'num_workers': 8, 'fold': 0, 'infer_overlap': 0.6, 'learning_rate': 0.0001, 'max_epochs': 10, 'pretrained_model': '', 'seed': 50, 'sw_batch_size': 1, 'val_every': 2, 'weight_decay': 1e-05}\n",
      "Paths Config: {'dataset_file': '/home/fahim/F2NetViT/Dataset/BraTS2023_2017_GLI_Mapping.xlsx', 'json_file': '/home/fahim/F2NetViT/Dataset/dataset.json', 'test_patient': '/home/fahim/F2NetViT/Dataset/BraTS2024-BraTS-GLI-TrainingData/training_data1_v2/BraTS-GLI-03064-100/', 'train_path': '/home/fahim/F2NetViT/Dataset/BraTS2024-BraTS-GLI-TrainingData/training_data1_v2/', 'validation_path': '/home/fahim/F2NetViT/Dataset/BraTS2024-BraTS-GLI-ValidationData/'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Manually define the project root\n",
    "project_root = Path('/home/fahim/F2NetViT')  # Set this to your project's root directory\n",
    "config_path = project_root / 'config'\n",
    "\n",
    "# Example: Load a YAML configuration\n",
    "import yaml\n",
    "\n",
    "with open(config_path / 'model/default.yaml') as f:\n",
    "    model_config = yaml.safe_load(f)\n",
    "# Load all configurations\n",
    "\n",
    "with open(config_path / 'dataset/default.yaml') as f:\n",
    "    dataset_config = yaml.safe_load(f)\n",
    "\n",
    "with open(config_path / 'training/default.yaml') as f:\n",
    "    training_config = yaml.safe_load(f)\n",
    "\n",
    "with open(config_path / 'paths/default.yaml') as f:\n",
    "    paths_config = yaml.safe_load(f)\n",
    "    \n",
    "# Use these configurations in your script\n",
    "print(f\"Model Config: {model_config}\")\n",
    "print(f\"Dataset Config: {dataset_config}\")\n",
    "print(f\"Training Config: {training_config}\")\n",
    "print(f\"Paths Config: {paths_config}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example access\n",
    "root_dir = paths_config.get('root_dir')\n",
    "train_root_dir = dataset_config.get('train_root_dir')\n",
    "\n",
    "# Use these configurations in your script\n",
    "print(f\"Model Config: {model_config}\")\n",
    "print(f\"Dataset Config: {dataset_config}\")\n",
    "print(f\"Training Config: {training_config}\")\n",
    "print(f\"Paths Config: {paths_config}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Modules and packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 12:51:31.678970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 12:51:31.715393: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 12:51:31.726684: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 12:51:31.748552: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 12:51:32.866130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKING /home/fahim/F2NetViT/Dataset/BraTS2024-BraTS-GLI-TrainingData/training_data1_v2/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import nibabel as nib\n",
    "import tqdm as tqdm\n",
    "from utils.meter import AverageMeter\n",
    "from utils.general import save_checkpoint, load_pretrained_model, resume_training\n",
    "from Data.dataloader import BraTSDataset, get_dataloader\n",
    "\n",
    "import monai\n",
    "from monai.data import create_test_image_3d, Dataset, DataLoader, decollate_batch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    ")\n",
    "from functools import partial\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logger\n",
    "import logging\n",
    "import hydra\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "file_handler = logging.FileHandler(filename= \"logger.log\")\n",
    "stream_handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(fmt= \"%(asctime)s: %(message)s\", datefmt= '%Y-%m-%d %H:%M:%S')\n",
    "file_handler.setFormatter(formatter)\n",
    "stream_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_args():\n",
    "    \"\"\"read commmand line arguments\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"command line args\")\n",
    "    parser.add_argument('--data', default= \"\", type= str, help= \"dataset root directory path\")\n",
    "    parser.add_argument('--fold', default= 0, type = int, help=\"folder name or number\")\n",
    "    parser.add_argument('--json_file', default= \"\", type = str, help =\"path to json file for splitting train and val folds\")\n",
    "    parser.add_argument('--batch', default=1, type= int, help= \"batch size\")\n",
    "    parser.add_argument('--img_roi', default=128, type = int, help = 'image roi size')\n",
    "    parser.add_argument('--val_every', default= 2, type = int, help= \"validate every 2 epochs\")\n",
    "    parser.add_argument('--max_epochs', default= 100, type= int, help= \"maximum number of epoch to train\")\n",
    "    parser.add_argument('--workers', default=2, type = int, help= \"Number of data loading workers\")\n",
    "    parser.add_argument('--pretrained_model', default= \"\", type = str, help = \"path to pretraiend model\")\n",
    "    parser.add_argument('--pretrained', action= 'store_true', help= \"use pretrained weights.\")\n",
    "    parser.add_argument('--resume', action= 'store_true', help=\"starting training from the saved ckpt.\")\n",
    "    parser.add_argument('--colab', action='store_true', help=\"colab, configure paths on drive\")\n",
    "    opt = parser.parse_args()\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_func, epoch, max_epochs = 100):\n",
    "    \"\"\"\n",
    "    train the model for epoch on MRI image and given ground truth labels\n",
    "    using set of arguments\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "    loader: torch.utils.data.Dataset\n",
    "    optimizer: torch.optim.adamw.AdamW\n",
    "    loss_func: monai.losses.dice.DiceLoss\n",
    "    epoch: int\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train() \n",
    "    tic = time.time()\n",
    "    run_loss = AverageMeter()\n",
    "    for index, batch_data in enumerate(loader):\n",
    "        logits = model(batch_data[\"image\"].to(device))\n",
    "        loss = loss_func(logits, batch_data[\"label\"].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        run_loss.update(loss.item(), n = batch_data[\"image\"].shape[0])\n",
    "        print(\n",
    "            \"Epoch {}/{} {}/{}\".format(epoch, max_epochs, index, len(loader)),\n",
    "            \"loss: {:.4f}\".format(run_loss.avg),\n",
    "            \"time {:.2f}s\".format(time.time() - tic))\n",
    "        print()\n",
    "        tic = time.time()\n",
    "    return run_loss.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a Validation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, loader, acc_func,\n",
    "        max_epochs = None, epoch = None, model_inferer = None,\n",
    "        post_sigmoid = None, post_pred = None):\n",
    "    \"\"\"\n",
    "    Validation phase\n",
    "    use model and validation dataset to validate the model performance on \n",
    "    validation dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "    loader: torch.util.data.Dataset\n",
    "    acc_func: monai.metrics.meandice.DiceMetric \n",
    "    num_epochs: int\n",
    "    epochs: int\n",
    "    model_inferer: nn.Module\n",
    "    post_sigmoid: monai.transforms.post.array.Activations\n",
    "    post_pred:monai.transforms.post.array.AsDiscrete\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    tic = time.time()\n",
    "    run_acc = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for index, batch_data in enumerate(loader):\n",
    "            logits = model_inferer(batch_data[\"image\"].to(device))\n",
    "            masks = decollate_batch(batch_data[\"label\"].to(device)) \n",
    "            prediction_lists = decollate_batch(logits)\n",
    "            predictions = [post_pred(post_sigmoid(prediction)) for prediction in prediction_lists]\n",
    "            acc_func.reset()\n",
    "            acc_func(y_pred = predictions, y = masks)\n",
    "            acc, not_nans = acc_func.aggregate()\n",
    "            run_acc.update(acc.cpu().numpy(), n = not_nans.cpu().numpy())\n",
    "            dice_tc = run_acc.avg[0]\n",
    "            dice_wt = run_acc.avg[1]\n",
    "            dice_et = run_acc.avg[2]\n",
    "            print(\n",
    "                \"Val {}/{} {}/{}\".format(epoch, max_epochs, index, len(loader)),\n",
    "                \", dice_tc:\",\n",
    "                dice_tc,\n",
    "                \", dice_wt:\",\n",
    "                dice_wt,\n",
    "                \", dice_et:\",\n",
    "                dice_et,\n",
    "                \", time {:.2f}s\".format(time.time() - tic),\n",
    "            )\n",
    "            tic = time.time()\n",
    "    return run_acc.avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save training data for later use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(training_loss,\n",
    "              et, wt, tc,\n",
    "              val_mean_acc,\n",
    "              val_losses,\n",
    "              training_dices,\n",
    "              epochs):\n",
    "    \"\"\"\n",
    "    save the training data for later use\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_loss: list\n",
    "    et: list\n",
    "    wt: list\n",
    "    tc: list\n",
    "    val_mean_acc: list\n",
    "    val_losses: list\n",
    "    tarining_dices: list,\n",
    "    epochs: list\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    NAMES = [\"training_loss\", \"WT\", \"ET\", \"TC\", \"mean_dice\", \"epochs\"]\n",
    "    data_lists = [training_loss, wt, et, tc, val_mean_acc, epochs]\n",
    "    for i in range(len(NAMES)):\n",
    "        data[f\"{NAMES[i]}\"] = data_lists[i]\n",
    "    data_df = pd.DataFrame(data)\n",
    "    data_df.to_csv('training_data.csv')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a function which trains and validates the model by using:\n",
    "1. the train and vaal loader. - Using torch.utils.data.Dataset\n",
    "2. Adam optimizer.            - Using torch.optim\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            optimizer,\n",
    "            loss_func,\n",
    "            acc_func,\n",
    "            scheduler,\n",
    "            max_epochs = 100,\n",
    "            model_inferer = None,\n",
    "            start_epoch = 0,\n",
    "            post_sigmoid = None,\n",
    "            post_pred = None,\n",
    "            val_every = 2):\n",
    "    \"\"\"\n",
    "    train and validate the model\n",
    "\n",
    "    model: nn.Module\n",
    "    train_loader: torch.utils.data.Dataset\n",
    "    val_loader: torch.utils.data.Dataset\n",
    "    optimizer: torch.optim\n",
    "    loss_func: monai.losses.dice.DiceLoss\n",
    "    acc_func:  monai.metrics.meandice.DiceMetric \n",
    "    schedular: torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "    max_epochs: int\n",
    "    model_inferer: nn.Module\n",
    "    start_epoch: int\n",
    "    post_sigmoid: monai.transforms.post.array.Activations\n",
    "    post_pred: monai.transforms.post.array.AsDiscrete\n",
    "    \"\"\"\n",
    "    val_acc_max = 0\n",
    "    dices_tc = []\n",
    "    dices_wt = []\n",
    "    dices_et = []\n",
    "    mean_dices = []\n",
    "    epoch_losses = [] # training loss\n",
    "    train_epochs = []\n",
    "    for epoch in range(start_epoch, max_epochs):\n",
    "        print()\n",
    "        print(time.ctime(), \"Epoch: \", epoch)\n",
    "        epoch_time = time.time()\n",
    "        training_loss = train_epoch(model=model,\n",
    "                                    loader= train_loader,\n",
    "                                    optimizer=optimizer,\n",
    "                                    loss_func= loss_func,\n",
    "                                    epoch= epoch,\n",
    "                                    max_epochs=max_epochs)\n",
    "        print(\n",
    "            \"Final training  {}/{}\".format(epoch + 1, max_epochs - 1),\n",
    "            \"loss: {:.4f}\".format(training_loss),\n",
    "            \"time {:.2f}s\".format(time.time() - epoch_time),\n",
    "        )\n",
    "\n",
    "        if epoch % val_every == 0 or epoch == 0:\n",
    "            epoch_losses.append(training_loss)\n",
    "            train_epochs.append(int(epoch))\n",
    "            val_epoch_time = time.time()\n",
    "            val_acc =  val(model= model,\n",
    "                          loader= val_loader,\n",
    "                          acc_func= acc_func,\n",
    "                          max_epochs= max_epochs,\n",
    "                          epoch = epoch,\n",
    "                          model_inferer= model_inferer,\n",
    "                          post_sigmoid=post_sigmoid,\n",
    "                          post_pred=post_pred)\n",
    "            dice_tc = val_acc[0]\n",
    "            dice_wt = val_acc[1]\n",
    "            dice_et = val_acc[2]\n",
    "            val_mean_acc = np.mean(val_acc)\n",
    "            print(\n",
    "                \"Final validation stats {}/{}\".format(epoch + 1, max_epochs - 1),\n",
    "                \", dice_tc:\",\n",
    "                dice_tc,\n",
    "                \", dice_wt:\",\n",
    "                dice_wt,\n",
    "                \", dice_et:\",\n",
    "                dice_et,\n",
    "                \", Dice_Avg:\",\n",
    "                val_mean_acc,\n",
    "                \", time {:.2f}s\".format(time.time() - epoch_time),\n",
    "            )\n",
    "            dices_tc.append(dice_tc)\n",
    "            dices_et.append(dice_et)\n",
    "            dices_wt.append(dices_wt)\n",
    "            mean_dices.append(val_mean_acc)\n",
    "            if val_mean_acc > val_acc_max:\n",
    "                print(\"new best ({:.6f} --> {:.6f}). \".format(val_acc_max, val_mean_acc))\n",
    "                val_acc_max = val_mean_acc\n",
    "                save_checkpoint(model=model,\n",
    "                                epoch= epoch,\n",
    "                                best_acc=val_acc_max)\n",
    "            scheduler.step()\n",
    "    print(\"Training Finished !, Best Accuracy: \", val_acc_max)\n",
    "    save_data(training_loss=training_loss,\n",
    "              et= dices_et,\n",
    "              wt= dices_wt,\n",
    "              tc=dices_tc,\n",
    "              val_mean_acc=mean_dices,\n",
    "              epochs=train_epochs)\n",
    "    \n",
    "    return (\n",
    "        val_acc_max,\n",
    "        dices_tc,\n",
    "        dices_wt,\n",
    "        dices_et,\n",
    "        mean_dices,\n",
    "        training_loss,\n",
    "        train_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this trains the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args, model,\n",
    "        loss_func,\n",
    "        acc_func,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        scheduler,\n",
    "        model_inferer = None,\n",
    "        post_sigmoid = None, \n",
    "        post_pred = None,\n",
    "        max_epochs = 100,\n",
    "        start_epoch = 0,\n",
    "        val_every = 2\n",
    "        ):\n",
    "    '''Now train the model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    args: argparse.parser\n",
    "    model: nn.Module\n",
    "    acc_func:  monai.metrics.meandice.DiceMetric\n",
    "    loss_func: monai.losses.dice.DiceLoss\n",
    "    optimizer: torch.optim.adamw.AdamW\n",
    "    train_loader: torch.utils.data.Dataset\n",
    "    val_loader: torch.utils.data.Dataset\n",
    "    schedular:  torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "    model_inferer: nn.Module\n",
    "    post_sigmoid: monai.transforms.post.array.Activations\n",
    "    post_pred:monai.transforms.post.array.AsDiscrete\n",
    "    max_epochs: int\n",
    "    start_epoch: int\n",
    "    val_every: int\n",
    "    '''\n",
    "    if args.pretrained:\n",
    "        print('Loading a pretrained model')\n",
    "        print()\n",
    "        model = load_pretrained_model(model, args.pretrained_model)\n",
    "    if args.resume:\n",
    "        print('Resuming training...')\n",
    "        model = resume_training(model, args.pretrained_model)\n",
    "    elif args.pretrained:\n",
    "        print(\"Using a pretrained model...\")\n",
    "    else:\n",
    "        print('Trainig from scrath!')\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print()\n",
    "    print(\"Total parameters count\", total_params)\n",
    "\n",
    "    (\n",
    "    val_mean_dice_max,\n",
    "    dices_tc,\n",
    "    dices_wt,\n",
    "    dices_et,\n",
    "    dices_mean,\n",
    "    train_losses,\n",
    "    train_epochs,\n",
    "    ) = trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_func=loss_func,\n",
    "        acc_func=acc_func,\n",
    "        scheduler=scheduler,\n",
    "        model_inferer=model_inferer,\n",
    "        start_epoch=start_epoch,\n",
    "        post_sigmoid=post_sigmoid,\n",
    "        post_pred=post_pred,\n",
    "    )\n",
    "    print()\n",
    "    logger.info(f\"train completed, best average dice: {val_mean_dice_max:.4f} \")\n",
    "    return (val_mean_dice_max, \n",
    "            dices_tc,\n",
    "            dices_wt,\n",
    "            dices_et,\n",
    "            dices_mean,\n",
    "            train_losses,\n",
    "            train_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIATE TRAINING!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [--help] [--hydra-help] [--version]\n",
      "                             [--cfg {job,hydra,all}] [--resolve]\n",
      "                             [--package PACKAGE] [--run] [--multirun]\n",
      "                             [--shell-completion] [--config-path CONFIG_PATH]\n",
      "                             [--config-name CONFIG_NAME]\n",
      "                             [--config-dir CONFIG_DIR]\n",
      "                             [--experimental-rerun EXPERIMENTAL_RERUN]\n",
      "                             [--info [{all,config,defaults,defaults-tree,plugins,searchpath}]]\n",
      "                             [overrides ...]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/fahim/.local/share/jupyter/runtime/kernel-v2-2131028EJ74sLCWKb4D.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fahim/.miniforge3/envs/brats_segmentation/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "@hydra.main(config_name='configs', config_path= 'conf', version_base=None)\n",
    "def main(cfg: DictConfig):\n",
    "    logging.info(f'Configs: {OmegaConf.to_yaml(cfg)}')\n",
    "    # read command line args\n",
    "    args = read_args()\n",
    "    \n",
    "    # set cuda if available and use CuDNN for efficient NN training\n",
    "    start_epoch = 0\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # post processing \n",
    "    post_pred = AsDiscrete(argmax= False, threshold = 0.5)\n",
    "    post_sigmoid = Activations(sigmoid= True)\n",
    "    \n",
    "    # define model \n",
    "    roi = cfg.model.roi\n",
    "    model = SwinUNETR(\n",
    "                    img_size=roi,\n",
    "                    in_channels=4,\n",
    "                    out_channels=3,\n",
    "                    feature_size=48,\n",
    "                    drop_rate=0.0,\n",
    "                    attn_drop_rate=0.0,\n",
    "                    dropout_path_rate=0.0,\n",
    "                    use_checkpoint=True,\n",
    "                            ).to(device)\n",
    "    \n",
    "    model_inferer = partial(\n",
    "                        sliding_window_inference,\n",
    "                        roi_size=[roi] * 3,\n",
    "                        sw_batch_size=cfg.training.sw_batch_size,\n",
    "                        predictor=model,\n",
    "                        overlap=cfg.model.infer_overlap)\n",
    "    \n",
    "    val_every = args.val_every\n",
    "\n",
    "    # loss function (dice loss for semantic segmentation)\n",
    "    loss_func = DiceLoss(to_onehot_y=False, sigmoid=True)\n",
    "\n",
    "    # Dice metric for performance evaluation\n",
    "    acc_func =  DiceMetric(include_background=True, reduction=MetricReduction.MEAN_BATCH, \n",
    "                                      get_not_nans=True)\n",
    "    \n",
    "    # default optimizer (experiment with other ones)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr= cfg.training.learning_rate, \n",
    "                                              weight_decay=cfg.training.weight_decay)\n",
    "    \n",
    "     # set maximum training epochs\n",
    "    max_epochs = args.max_epochs if args.max_epochs else cfg.training.max_epochs\n",
    "\n",
    "    # Cosine Annearling learning rate schedular \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max= max_epochs)\n",
    "    \n",
    "    # configure batch size and workers\n",
    "    dataset_info_csv = cfg.paths.dataset_file \n",
    "    batch_size = args.batch if args.batch else cfg.training.batch_size\n",
    "    num_workers = args.workers if args.workers else cfg.training.num_workers\n",
    "    \n",
    "    # if using Google colab to access drive or other platform please configure \n",
    "    # paths belows\n",
    "    if args.colab:\n",
    "        train_dir = cfg.colab.train_path\n",
    "        dataset_info_csv = cfg.colab.dataset_file\n",
    "        json_file =cfg.colab.json_file\n",
    "    else:\n",
    "        train_dir = cfg.paths.train_path\n",
    "        dataset_info_csv = cfg.paths.dataset_file\n",
    "        json_file = cfg.paths.json_file\n",
    "\n",
    "    logger.info(\"Configured. Now Loading the dataset...\\n\")\n",
    "\n",
    "    # load training and validation datasets\n",
    "    train_loader = get_dataloader(BraTSDataset, \n",
    "                                  dataset_info_csv, \n",
    "                                  phase = \"train\",\n",
    "                                  batch_size = batch_size, \n",
    "                                  num_workers = num_workers,\n",
    "                                  json_file = json_file,\n",
    "                                  fold = args.fold,\n",
    "                                  train_dir = train_dir)\n",
    "    # validation data loader\n",
    "    val_loader = get_dataloader(BraTSDataset, \n",
    "                                dataset_info_csv, \n",
    "                                phase= \"val\", \n",
    "                                batch_size = batch_size,  \n",
    "                                num_workers = num_workers,\n",
    "                                json_file = json_file,\n",
    "                                fold = args.fold, \n",
    "                                train_dir = train_dir)\n",
    "    \n",
    "    logger.info('starting training...')\n",
    "\n",
    "    # run training\n",
    "    run(args, model=model,\n",
    "        loss_func= loss_func,\n",
    "        acc_func= acc_func,\n",
    "        optimizer= optimizer,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        scheduler=scheduler,\n",
    "        model_inferer=model_inferer,\n",
    "        post_sigmoid=post_sigmoid,\n",
    "        post_pred=post_pred,\n",
    "        max_epochs=max_epochs,\n",
    "        start_epoch=start_epoch,\n",
    "        val_every=val_every)\n",
    "    \n",
    "    logger.info('Training complete!!!')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py --weights /content/model0.pt --fold 0 --workers 2 --batch 1 --json_file /content/brats21_folds.json --platform_changed\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-30 13:05:56.807393: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 13:05:56.843115: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 13:05:56.854167: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 13:05:56.875317: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 13:05:58.084266: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "CHECKING /home/fahim/F2NetViT/Dataset/BraTS2024-BraTS-GLI-TrainingData/training_data1_v2/\n",
      "usage: train.py [--help] [--hydra-help] [--version] [--cfg {job,hydra,all}]\n",
      "                [--resolve] [--package PACKAGE] [--run] [--multirun]\n",
      "                [--shell-completion] [--config-path CONFIG_PATH]\n",
      "                [--config-name CONFIG_NAME] [--config-dir CONFIG_DIR]\n",
      "                [--experimental-rerun EXPERIMENTAL_RERUN]\n",
      "                [--info [{all,config,defaults,defaults-tree,plugins,searchpath}]]\n",
      "                [overrides ...]\n",
      "train.py: error: unrecognized arguments: --fold --batch 1 --val_every 1 --max_epochs 2 --pretrained_model /content/model0.pt --pretrained --workers 2 --platform_changed\n"
     ]
    }
   ],
   "source": [
    "!python train.py --fold 0  --batch 1 --val_every 1 --max_epochs 2 --pretrained_model /content/model0.pt --pretrained --workers 2 --platform_changed\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brats_segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
